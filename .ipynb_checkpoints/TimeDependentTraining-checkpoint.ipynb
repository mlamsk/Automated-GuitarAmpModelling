{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfa3dd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Parameterized Data With Distinct Validation\n",
      "['./Recordings/hverb/H-Verb_Training_Clean.wav', './Recordings/hverb/H-Verb_Training_Dirty.wav', './Recordings/hverb/H-Verb_Test_Clean.wav', './Recordings/hverb/H-Verb_Test_Dirty.wav', './Recordings/hverb/H-Verb_Test_Clean.wav', './Recordings/hverb/H-Verb_Test_Dirty.wav']\n",
      "/home/micahtseng/Personal/mlamsk/python3Env/pytorchSandbox/Automated-GuitarAmpModelling/prep_wav.py:138: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  in_rate, in_data = wavfile.read(args.validation[0])\n",
      "/home/micahtseng/Personal/mlamsk/python3Env/pytorchSandbox/Automated-GuitarAmpModelling/prep_wav.py:139: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  out_rate, out_data = wavfile.read(args.validation[1])\n",
      "/home/micahtseng/Personal/mlamsk/python3Env/pytorchSandbox/Automated-GuitarAmpModelling/prep_wav.py:143: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  in_rate, in_data = wavfile.read(args.validation[2])\n",
      "/home/micahtseng/Personal/mlamsk/python3Env/pytorchSandbox/Automated-GuitarAmpModelling/prep_wav.py:144: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  out_rate, out_data = wavfile.read(args.validation[3])\n",
      "/home/micahtseng/Personal/mlamsk/python3Env/pytorchSandbox/Automated-GuitarAmpModelling/prep_wav.py:148: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  in_rate, in_data = wavfile.read(args.validation[4])\n",
      "/home/micahtseng/Personal/mlamsk/python3Env/pytorchSandbox/Automated-GuitarAmpModelling/prep_wav.py:149: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  out_rate, out_data = wavfile.read(args.validation[5])\n"
     ]
    }
   ],
   "source": [
    "!python prep_wav.py \"hverb-noskip\" -s \\\n",
    "\"./Recordings/hverb/H-Verb_Training_Clean.wav\" \\\n",
    "\"./Recordings/hverb/H-Verb_Training_Dirty.wav\" \\\n",
    "\"./Recordings/hverb/H-Verb_Test_Clean.wav\" \\\n",
    "\"./Recordings/hverb/H-Verb_Test_Dirty.wav\" \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ea78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no saved model found, creating new network\n",
      "cuda device available\n",
      "unimplemented audio data type conversion...\n",
      "unimplemented audio data type conversion...\n",
      "unimplemented audio data type conversion...\n",
      "unimplemented audio data type conversion...\n",
      "Epoch:  1\n",
      "current learning rate: 0.005\n",
      "Epoch:  2\n",
      "Val loss: tensor(2.6835)\n",
      "current learning rate: 0.005\n",
      "Epoch:  3\n",
      "current learning rate: 0.005\n",
      "Epoch:  4\n",
      "Val loss: tensor(1.5080)\n",
      "current learning rate: 0.005\n",
      "Epoch:  5\n",
      "current learning rate: 0.005\n",
      "Epoch:  6\n",
      "Val loss: tensor(1.1784)\n",
      "current learning rate: 0.005\n",
      "Epoch:  7\n",
      "current learning rate: 0.005\n",
      "Epoch:  8\n",
      "Val loss: tensor(0.9885)\n",
      "current learning rate: 0.005\n",
      "Epoch:  9\n",
      "current learning rate: 0.005\n",
      "Epoch:  10\n",
      "Val loss: tensor(0.8806)\n",
      "current learning rate: 0.005\n",
      "Epoch:  11\n",
      "current learning rate: 0.005\n",
      "Epoch:  12\n",
      "Val loss: tensor(0.8247)\n",
      "current learning rate: 0.005\n",
      "Epoch:  13\n",
      "current learning rate: 0.005\n",
      "Epoch:  14\n",
      "Val loss: tensor(0.7971)\n",
      "current learning rate: 0.005\n",
      "Epoch:  15\n",
      "current learning rate: 0.005\n",
      "Epoch:  16\n",
      "Val loss: tensor(0.7849)\n",
      "current learning rate: 0.005\n",
      "Epoch:  17\n",
      "current learning rate: 0.005\n",
      "Epoch:  18\n",
      "Val loss: tensor(0.7789)\n",
      "current learning rate: 0.005\n",
      "Epoch:  19\n",
      "current learning rate: 0.005\n",
      "Epoch:  20\n",
      "Val loss: tensor(0.7748)\n",
      "current learning rate: 0.005\n",
      "Epoch:  21\n",
      "current learning rate: 0.005\n",
      "Epoch:  22\n",
      "Val loss: tensor(0.7720)\n",
      "current learning rate: 0.005\n",
      "Epoch:  23\n",
      "current learning rate: 0.005\n",
      "Epoch:  24\n",
      "Val loss: tensor(0.7682)\n",
      "current learning rate: 0.005\n",
      "Epoch:  25\n",
      "current learning rate: 0.005\n",
      "Epoch:  26\n",
      "Val loss: tensor(0.7651)\n",
      "current learning rate: 0.005\n",
      "Epoch:  27\n",
      "current learning rate: 0.005\n",
      "Epoch:  28\n",
      "Val loss: tensor(0.8560)\n",
      "current learning rate: 0.005\n",
      "Epoch:  29\n",
      "current learning rate: 0.005\n",
      "Epoch:  30\n",
      "Val loss: tensor(0.8072)\n",
      "current learning rate: 0.005\n",
      "Epoch:  31\n",
      "current learning rate: 0.005\n",
      "Epoch:  32\n",
      "Val loss: tensor(0.7627)\n",
      "current learning rate: 0.005\n",
      "Epoch:  33\n",
      "current learning rate: 0.005\n",
      "Epoch:  34\n",
      "Val loss: tensor(0.7519)\n",
      "current learning rate: 0.005\n",
      "Epoch:  35\n",
      "current learning rate: 0.005\n",
      "Epoch:  36\n",
      "Val loss: tensor(0.7474)\n",
      "current learning rate: 0.005\n",
      "Epoch:  37\n",
      "current learning rate: 0.005\n",
      "Epoch:  38\n",
      "Val loss: tensor(0.7470)\n",
      "current learning rate: 0.005\n",
      "Epoch:  39\n",
      "current learning rate: 0.005\n",
      "Epoch:  40\n",
      "Val loss: tensor(0.7466)\n",
      "current learning rate: 0.005\n",
      "Epoch:  41\n",
      "current learning rate: 0.005\n",
      "Epoch:  42\n",
      "Val loss: tensor(0.7463)\n",
      "current learning rate: 0.005\n",
      "Epoch:  43\n",
      "current learning rate: 0.005\n",
      "Epoch:  44\n",
      "Val loss: tensor(0.7455)\n",
      "current learning rate: 0.005\n",
      "Epoch:  45\n",
      "current learning rate: 0.005\n",
      "Epoch:  46\n",
      "Val loss: tensor(0.7446)\n",
      "current learning rate: 0.005\n",
      "Epoch:  47\n",
      "current learning rate: 0.005\n",
      "Epoch:  48\n",
      "Val loss: tensor(0.7439)\n",
      "current learning rate: 0.005\n",
      "Epoch:  49\n",
      "current learning rate: 0.005\n",
      "Epoch:  50\n"
     ]
    }
   ],
   "source": [
    "!python dist_model_recnet.py -l RNN3-hverb-noskip -eps 200 -bs 10 -sc 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f8b7602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136868/1959163908.py:9: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  rate, data = wavfile.read(\"./Recordings/hverb/H-Verb_Training_Dirty.wav\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "\n",
    "def save_wav(name, data):\n",
    "    wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
    "\n",
    "rate, data = wavfile.read(\"./Recordings/hverb/H-Verb_Training_Dirty.wav\")\n",
    "\n",
    "data = data.astype(np.float32).flatten()\n",
    "\n",
    "shuffle = torch.randperm(data.shape[0])\n",
    "\n",
    "bs = 88200\n",
    "\n",
    "for batch_i in range(math.ceil(shuffle.shape[0] / bs)):\n",
    "    data_batch = data[batch_i * bs: (batch_i + 1)*bs]\n",
    "    a = \"./test/\"+str(batch_i)+\".wav\"\n",
    "    save_wav(a, data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af92a093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d78b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: dist_model_recnet.py [-h] [--device DEVICE]\r\n",
      "                            [--data_location DATA_LOCATION]\r\n",
      "                            [--file_name FILE_NAME]\r\n",
      "                            [--load_config LOAD_CONFIG]\r\n",
      "                            [--config_location CONFIG_LOCATION]\r\n",
      "                            [--save_location SAVE_LOCATION]\r\n",
      "                            [--load_model LOAD_MODEL] [--seed SEED]\r\n",
      "                            [--segment_length SEGMENT_LENGTH]\r\n",
      "                            [--epochs EPOCHS] [--validation_f VALIDATION_F]\r\n",
      "                            [--validation_p VALIDATION_P]\r\n",
      "                            [--batch_size BATCH_SIZE] [--iter_num ITER_NUM]\r\n",
      "                            [--learn_rate LEARN_RATE] [--init_len INIT_LEN]\r\n",
      "                            [--up_fr UP_FR] [--cuda CUDA]\r\n",
      "                            [--loss_fcns LOSS_FCNS] [--pre_filt PRE_FILT]\r\n",
      "                            [--val_chunk VAL_CHUNK] [--test_chunk TEST_CHUNK]\r\n",
      "                            [--model MODEL] [--input_size INPUT_SIZE]\r\n",
      "                            [--output_size OUTPUT_SIZE]\r\n",
      "                            [--num_blocks NUM_BLOCKS]\r\n",
      "                            [--hidden_size HIDDEN_SIZE]\r\n",
      "                            [--unit_type UNIT_TYPE] [--skip_con SKIP_CON]\r\n",
      "\r\n",
      "This script implements training for neural network amplifier/distortion\r\n",
      "effects modelling. This is intended to recreate the training of models of the\r\n",
      "ht1 amplifier and big muff distortion pedal, but can easily be adapted to use\r\n",
      "any dataset\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --device DEVICE, -p DEVICE\r\n",
      "                        This label describes what device is being modelled\r\n",
      "  --data_location DATA_LOCATION, -dl DATA_LOCATION\r\n",
      "                        Location of the \"Data\" directory\r\n",
      "  --file_name FILE_NAME, -fn FILE_NAME\r\n",
      "                        The filename of the wav file to be loaded as the\r\n",
      "                        input/target data, the script looks for fileswith the\r\n",
      "                        filename and the extensions -input.wav and -target.wav\r\n",
      "  --load_config LOAD_CONFIG, -l LOAD_CONFIG\r\n",
      "                        File path, to a JSON config file, arguments listed in\r\n",
      "                        the config file will replace the defaults\r\n",
      "  --config_location CONFIG_LOCATION, -cl CONFIG_LOCATION\r\n",
      "                        Location of the \"Configs\" directory\r\n",
      "  --save_location SAVE_LOCATION, -sloc SAVE_LOCATION\r\n",
      "                        Directory where trained models will be saved\r\n",
      "  --load_model LOAD_MODEL, -lm LOAD_MODEL\r\n",
      "                        load a pretrained model if it is found\r\n",
      "  --seed SEED           seed all of the random number generators if desired\r\n",
      "  --segment_length SEGMENT_LENGTH, -slen SEGMENT_LENGTH\r\n",
      "                        Training audio segment length in samples\r\n",
      "  --epochs EPOCHS, -eps EPOCHS\r\n",
      "                        Max number of training epochs to run\r\n",
      "  --validation_f VALIDATION_F, -vfr VALIDATION_F\r\n",
      "                        Validation Frequency (in epochs)\r\n",
      "  --validation_p VALIDATION_P, -vp VALIDATION_P\r\n",
      "                        How many validations without improvement before\r\n",
      "                        stopping training, None for no early stopping\r\n",
      "  --batch_size BATCH_SIZE, -bs BATCH_SIZE\r\n",
      "                        Training mini-batch size\r\n",
      "  --iter_num ITER_NUM, -it ITER_NUM\r\n",
      "                        Overrides --batch_size and instead sets the batch_size\r\n",
      "                        so that a total of --iter_num batchesare processed in\r\n",
      "                        each epoch\r\n",
      "  --learn_rate LEARN_RATE, -lr LEARN_RATE\r\n",
      "                        Initial learning rate\r\n",
      "  --init_len INIT_LEN, -il INIT_LEN\r\n",
      "                        Number of sequence samples to process before starting\r\n",
      "                        weight updates\r\n",
      "  --up_fr UP_FR, -uf UP_FR\r\n",
      "                        For recurrent models, number of samples to run in\r\n",
      "                        between updating network weights, i.e the default\r\n",
      "                        argument updates every 1000 samples\r\n",
      "  --cuda CUDA, -cu CUDA\r\n",
      "                        Use GPU if available\r\n",
      "  --loss_fcns LOSS_FCNS, -lf LOSS_FCNS\r\n",
      "                        Which loss functions, ESR, ESRPre, DC. Argument is a\r\n",
      "                        dictionary with each key representing aloss function\r\n",
      "                        name and the corresponding value being the\r\n",
      "                        multiplication factor applied to thatloss function,\r\n",
      "                        used to control the contribution of each loss function\r\n",
      "                        to the overall loss\r\n",
      "  --pre_filt PRE_FILT, -pf PRE_FILT\r\n",
      "                        FIR filter coefficients for pre-emphasis filter, can\r\n",
      "                        also read in a csv file\r\n",
      "  --val_chunk VAL_CHUNK, -vs VAL_CHUNK\r\n",
      "                        Number of sequence samples to processin each chunk of\r\n",
      "                        validation\r\n",
      "  --test_chunk TEST_CHUNK, -tc TEST_CHUNK\r\n",
      "                        Number of sequence samples to processin each chunk of\r\n",
      "                        validation\r\n",
      "  --model MODEL, -m MODEL\r\n",
      "                        model architecture\r\n",
      "  --input_size INPUT_SIZE, -is INPUT_SIZE\r\n",
      "                        1 for mono input data, 2 for stereo, etc\r\n",
      "  --output_size OUTPUT_SIZE, -os OUTPUT_SIZE\r\n",
      "                        1 for mono output data, 2 for stereo, etc\r\n",
      "  --num_blocks NUM_BLOCKS, -nb NUM_BLOCKS\r\n",
      "                        Number of recurrent blocks\r\n",
      "  --hidden_size HIDDEN_SIZE, -hs HIDDEN_SIZE\r\n",
      "                        Recurrent unit hidden state size\r\n",
      "  --unit_type UNIT_TYPE, -ut UNIT_TYPE\r\n",
      "                        LSTM or GRU or RNN\r\n",
      "  --skip_con SKIP_CON, -sc SKIP_CON\r\n",
      "                        is there a skip connection for the input to the output\r\n"
     ]
    }
   ],
   "source": [
    "!python ./dist_model_recnet.py -h\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
